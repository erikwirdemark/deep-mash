{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa61313",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62042999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Generator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import torchaudio.functional as AF\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deepmash.data_processing.constants import *\n",
    "from deepmash.data_processing.gtzan_stems import *\n",
    "from deepmash.data_processing.new_dataloader import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da39239",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 16000           # think its hard to go lower than 16kHz\n",
    "CHUNK_DURATION_SEC = 15     # all chunks will be exactly this long\n",
    "MIN_CHUNK_DURATION_SEC = 5  # discard chunks shorter than this (otherwise zero-pad to CHUNK_DURATION_SEC)\n",
    "\n",
    "# mel-spectrogram settings (using same as cocola for now)\n",
    "N_MELS = 64\n",
    "F_MIN = 60 \n",
    "F_MAX = 7800\n",
    "WINDOW_SIZE = 1024 # 64ms @ 16kHz (should be power of 2 for efficiency)\n",
    "HOP_SIZE = 320     # 20ms @ 16kHz\n",
    "\n",
    "INPUT_ROOT = \"path/to/gtzan/stems\"\n",
    "ORIGINALS_ROOT = \"path/to/gtzan/originals\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf3654e",
   "metadata": {},
   "source": [
    "### All functions taken from data_processing folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utility functions ----------\n",
    "\n",
    "def ensure_same_length(tensors: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    min_len = min(len(t) for t in tensors)\n",
    "    return [t[:min_len] for t in tensors]\n",
    "\n",
    "\n",
    "def ensure_2d(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.ndim == 1:\n",
    "        return x.unsqueeze(0)\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(f\"Input tensor has {x.ndim} dimensions, expected 1 or 2.\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def mix_stems(stems: list[torch.Tensor], peak_val=0.98) -> torch.Tensor:\n",
    "    stems = ensure_same_length(stems)\n",
    "    mixed: torch.Tensor = sum(stems)  # type: ignore\n",
    "    max_val = mixed.abs().max()\n",
    "    if max_val > 0:\n",
    "        mixed = mixed / max_val * peak_val\n",
    "    return mixed\n",
    "\n",
    "\n",
    "def zero_pad_or_clip(x: torch.Tensor, target_len: int) -> torch.Tensor:\n",
    "    if len(x) >= target_len:\n",
    "        return x[:target_len]\n",
    "    pad_len = target_len - len(x)\n",
    "    return torch.cat([x, torch.zeros(pad_len)], dim=0)\n",
    "\n",
    "\n",
    "def load_audio(path: Path | str, sr: int | float, frame_offset=0, num_frames=-1):\n",
    "    y, sr = torchaudio.load(path, frame_offset=frame_offset, num_frames=num_frames)\n",
    "    if y.shape[0] > 1:\n",
    "        y = y.mean(dim=0, keepdim=True)  # convert to mono\n",
    "    y = AF.resample(y, orig_freq=sr, new_freq=TARGET_SR)\n",
    "    return y  # (1, sr*duration)\n",
    "\n",
    "\n",
    "def get_chunks(vocals: torch.Tensor, non_vocals: torch.Tensor) -> Generator[tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    chunk_frames = CHUNK_DURATION_SEC * TARGET_SR\n",
    "    min_chunk_frames = MIN_CHUNK_DURATION_SEC * TARGET_SR\n",
    "\n",
    "    vocals, non_vocals = ensure_same_length([vocals, non_vocals])\n",
    "\n",
    "    for i, start in enumerate(range(0, len(vocals), chunk_frames)):\n",
    "        vocals_chunk = vocals[start:start + chunk_frames]\n",
    "        non_vocals_chunk = non_vocals[start:start + chunk_frames]\n",
    "\n",
    "        if len(vocals_chunk) < min_chunk_frames:\n",
    "            continue\n",
    "        vocals_chunk = zero_pad_or_clip(vocals_chunk, chunk_frames)\n",
    "        non_vocals_chunk = zero_pad_or_clip(non_vocals_chunk, chunk_frames)\n",
    "\n",
    "        yield vocals_chunk, non_vocals_chunk\n",
    "\n",
    "\n",
    "def get_gtzan_track_folders(root: Path | str):\n",
    "    return sorted(p for p in Path(root).glob(\"*/*\") if p.is_dir())\n",
    "\n",
    "# ---------- Mel transform ----------\n",
    "\n",
    "class ToLogMel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.to_melspec = AT.MelSpectrogram(\n",
    "            sample_rate=TARGET_SR,\n",
    "            n_mels=N_MELS,\n",
    "            n_fft=WINDOW_SIZE,\n",
    "            hop_length=HOP_SIZE,\n",
    "            f_min=F_MIN,\n",
    "            f_max=F_MAX,\n",
    "        )\n",
    "        self.to_db = AT.AmplitudeToDB()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.to_db(self.to_melspec(x))\n",
    "\n",
    "\n",
    "# ---------- Main dataset with preprocessing ----------\n",
    "\n",
    "class GTZANStemsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Path | str = INPUT_ROOT,\n",
    "        originals_root: Path | str = ORIGINALS_ROOT,\n",
    "        preprocess=True,\n",
    "        preprocess_transform: nn.Module | None = None,\n",
    "        runtime_transform: nn.Module | None = None,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.root = Path(root_dir)\n",
    "        self.originals_root = Path(originals_root)\n",
    "        self.processed_root = self.root.parent / (self.root.name + \"-processed\")\n",
    "\n",
    "        self.preprocess_transform = preprocess_transform\n",
    "        self.runtime_transform = runtime_transform\n",
    "        self.device = device\n",
    "\n",
    "        if preprocess:\n",
    "            print(f\"Preprocessing GTZAN stems from {self.root} and originals from {self.originals_root}\")\n",
    "            self._preprocess()\n",
    "\n",
    "        # After preprocessing, load the chunk list\n",
    "        self.chunk_dirs = sorted([p for p in self.processed_root.glob(\"*\") if p.is_dir()])\n",
    "\n",
    "    def _preprocess(self):\n",
    "        \"\"\"\n",
    "        Assuming input files like \"`self.root`/blues/blues.000001/{drums|bass|other|vocals}.wav\":\n",
    "        1. load as tensors\n",
    "        2. convert to mono if in stereo\n",
    "        3. resample (default 16kHz)\n",
    "        4. mix all non-vocal stems together and discard originals\n",
    "        5. chunk into `CHUNK_DURATION_SEC` (default 10s) segments, zero-pad last chunk if needed\n",
    "        6. apply optional `preprocess_transform` (e.g. mel-spectrogram), make sure shapes are correct\n",
    "        7. save as `self.processed_root`/blues.000001.chunk{1|2|...}/{non-vocals|vocals}.pt\n",
    "        \"\"\"\n",
    "        os.makedirs(self.processed_root, exist_ok=True)\n",
    "        track_folders = get_gtzan_track_folders(self.root)\n",
    "\n",
    "        for track_folder in tqdm(track_folders):\n",
    "            all_stem_paths = list(track_folder.glob(\"*.wav\"))\n",
    "            assert {p.stem for p in all_stem_paths} == {\"drums\", \"bass\", \"other\", \"vocals\"}, \\\n",
    "                f\"Missing stems for {track_folder}\"\n",
    "\n",
    "            vocals_path = [p for p in all_stem_paths if p.stem == \"vocals\"][0]\n",
    "            non_vocals_paths = [p for p in all_stem_paths if p.stem != \"vocals\"]\n",
    "\n",
    "            track_name = track_folder.name\n",
    "            genre = track_folder.parent.name  # e. g. \"blues\"\n",
    "            orig_path = self.originals_root / f\"{genre}\" / f\"{track_name}.wav\"\n",
    "            \n",
    "            # Load and mix stems\n",
    "            try:\n",
    "                vocals = load_audio(vocals_path, sr=TARGET_SR)\n",
    "                non_vocals = mix_stems([load_audio(p, sr=TARGET_SR) for p in non_vocals_paths])\n",
    "                original = load_audio(orig_path, sr=TARGET_SR)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Skipping track {track_name} due to loading error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            vocals = vocals.squeeze(0)\n",
    "            non_vocals = non_vocals.squeeze(0)\n",
    "            original = original.squeeze(0)\n",
    "\n",
    "            # Generate aligned chunks\n",
    "            for i, ((vocals_chunk, non_vocals_chunk), (orig_chunk, _)) in enumerate(\n",
    "                zip(get_chunks(vocals, non_vocals), get_chunks(original, original))\n",
    "            ):\n",
    "                if self.preprocess_transform is not None:\n",
    "                    with torch.no_grad():\n",
    "                        vocals_chunk = self.preprocess_transform(vocals_chunk.unsqueeze(0))  # (1, T)\n",
    "                        non_vocals_chunk = self.preprocess_transform(non_vocals_chunk.unsqueeze(0))\n",
    "                        orig_chunk = self.preprocess_transform(orig_chunk.unsqueeze(0))\n",
    "                        \n",
    "                chunk_folder = self.processed_root / f\"{track_name}.chunk{i+1}\"\n",
    "                os.makedirs(chunk_folder, exist_ok=True)\n",
    "                torch.save(vocals_chunk, chunk_folder / \"vocals.pt\")\n",
    "                torch.save(non_vocals_chunk, chunk_folder / \"non-vocals.pt\")\n",
    "                torch.save(orig_chunk, chunk_folder / \"original.pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunk_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_dir = self.chunk_dirs[idx]\n",
    "        vocals = torch.load(chunk_dir / \"vocals.pt\")\n",
    "        non_vocals = torch.load(chunk_dir / \"non-vocals.pt\") \n",
    "        original = torch.load(chunk_dir / \"original.pt\")\n",
    "\n",
    "        if self.runtime_transform:\n",
    "            vocals = self.runtime_transform(vocals)\n",
    "            non_vocals = self.runtime_transform(non_vocals)\n",
    "            original = self.runtime_transform(original)\n",
    "\n",
    "        return {\n",
    "            \"vocals\": vocals.to(self.device),\n",
    "            \"non_vocals\": non_vocals.to(self.device),\n",
    "            \"original\": original.to(self.device),\n",
    "            \"chunk_name\": chunk_dir.name,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c3b80f",
   "metadata": {},
   "source": [
    "To create the dataloaders needed to train our model we first need to split the dataset. How we do this is by using train_test_split() on unique track names so that chunks from the same audio files doesn't end up in different splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Split dataset by tracks ----------\n",
    "\n",
    "def split_dataset_by_tracks(dataset: GTZANStemsDataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "\n",
    "    # Extract unique track names\n",
    "    all_tracks = sorted({p.name.split(\".chunk\")[0] for p in dataset.chunk_dirs})\n",
    "    \n",
    "    train_tracks, temp_tracks = train_test_split(all_tracks, test_size=(1 - train_ratio), random_state=random_state)\n",
    "    val_tracks, test_tracks = train_test_split(temp_tracks, test_size=test_ratio / (test_ratio + val_ratio), random_state=random_state)\n",
    "\n",
    "    def get_chunk_indices(track_list):\n",
    "        return [i for i, chunk_dir in enumerate(dataset.chunk_dirs) if chunk_dir.name.split(\".chunk\")[0] in track_list]\n",
    "\n",
    "    train_dataset = Subset(dataset, get_chunk_indices(train_tracks))\n",
    "    val_dataset = Subset(dataset, get_chunk_indices(val_tracks))\n",
    "    test_dataset = Subset(dataset, get_chunk_indices(test_tracks))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# ---------- DataLoaders ----------\n",
    "\n",
    "def create_dataloaders(preprocess=False, batch_size=16, num_workers=2):\n",
    "    dataset = GTZANStemsDataset(preprocess=preprocess, preprocess_transform=ToLogMel())\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset_by_tracks(dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482da47",
   "metadata": {},
   "source": [
    "Below is a test to see the shapes of vocals, non-vocals, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, _ = create_dataloaders(batch_size=4)\n",
    "# Get one batch from the training dataloader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Print keys\n",
    "print(batch.keys())\n",
    "print(\"Vocals:\", batch['vocals'].shape) # [batch_size, channels, n_mels, time] if using Mel\n",
    "print(\"Non-vocals:\", batch['non_vocals'].shape)\n",
    "print(\"Original:\", batch['original'].shape)\n",
    "print(\"Chunk names:\", batch['chunk_name'])   # List of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844138fd",
   "metadata": {},
   "source": [
    "### Model and loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718f9aac",
   "metadata": {},
   "source": [
    "CAN BE CHANGED!!! \n",
    "\n",
    "I followed the project purposal and created a dual encoder for the vocal + instruments. Currently i have created a CNN + Transformer model for each MusicEncoder but you can simply remove the transformer layer and change the Projection layer if you want to not use transformers. \n",
    "\n",
    "For loss function it uses costrastive loss (Cosine similarity) to find the most similar matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d952557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicEncoder(nn.Module):\n",
    "    def __init__(self, embedding_dim=128, cnn_name='resnet18', pretrained=True, n_heads=4, n_layers=2):\n",
    "        super(MusicEncoder, self).__init__()\n",
    "\n",
    "        # --- Pretrained CNN feature extractor ---\n",
    "        base_cnn = getattr(models, cnn_name)(pretrained=pretrained)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_cnn.children())[:-2])\n",
    "        self.cnn_out_channels = 512  # resnet18 output channels\n",
    "\n",
    "        # --- Transformer encoder ---\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.cnn_out_channels,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=1024,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # --- Projection to embedding ---\n",
    "        self.fc = nn.Linear(self.cnn_out_channels, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, 1, n_mels, time]\n",
    "        \"\"\"\n",
    "        if x.shape[1] == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)  # [B,3,H,W] for ResNet\n",
    "\n",
    "        feats = self.feature_extractor(x)  # [B, C, H', W']\n",
    "        B, C, H, W = feats.shape\n",
    "\n",
    "        # Collapse frequency dimension\n",
    "        feats = feats.mean(dim=2)           # [B, C, time]\n",
    "        feats = feats.transpose(1, 2)       # [B, time, C]\n",
    "\n",
    "        feats = self.transformer(feats)     # temporal modeling\n",
    "        pooled = feats.mean(dim=1)          # global average pooling\n",
    "\n",
    "        emb = self.fc(pooled)\n",
    "        emb = F.normalize(emb, p=2, dim=1)  # L2-normalize\n",
    "        return emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d76a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualEncoderModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(DualEncoderModel, self).__init__()\n",
    "        self.vocal_encoder = MusicEncoder(embedding_dim)\n",
    "        self.instr_encoder = MusicEncoder(embedding_dim)\n",
    "\n",
    "    def forward(self, vocals, non_vocals):\n",
    "        v_emb = self.vocal_encoder(vocals)\n",
    "        i_emb = self.instr_encoder(non_vocals)\n",
    "        return v_emb, i_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d867958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(v_emb, i_emb, temperature=0.07):\n",
    "    \"\"\"\n",
    "    InfoNCE-style contrastive loss using cosine similarity between normalized embeddings.\n",
    "    \"\"\"\n",
    "    # Normalize embeddings\n",
    "    v_emb = F.normalize(v_emb, dim=-1)\n",
    "    i_emb = F.normalize(i_emb, dim=-1)\n",
    "\n",
    "    # Cosine similarity matrix scaled by temperature\n",
    "    sim_matrix = torch.matmul(v_emb, i_emb.T) / temperature  # [B, B]\n",
    "\n",
    "    # Targets: diagonal elements are positives\n",
    "    targets = torch.arange(v_emb.size(0), device=v_emb.device)\n",
    "\n",
    "    # Cross-entropy loss in both directions\n",
    "    loss_v2i = F.cross_entropy(sim_matrix, targets)\n",
    "    loss_i2v = F.cross_entropy(sim_matrix.T, targets)\n",
    "\n",
    "    return (loss_v2i + loss_i2v) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-4, save_path=\"dual_encoder_best.pth\", device=None):\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    print(f\"ðŸš€ Starting training on {device} for {epochs} epochs...\\n\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # -------------------- TRAIN --------------------\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [Train]\"):\n",
    "            vocals = batch[\"vocals\"].to(device)\n",
    "            non_vocals = batch[\"non_vocals\"].to(device)\n",
    "\n",
    "            v_emb, i_emb = model(vocals, non_vocals)\n",
    "            loss = contrastive_loss(v_emb, i_emb)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # -------------------- VALIDATE --------------------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch}/{epochs} [Val]\"):\n",
    "                vocals = batch[\"vocals\"].to(device)\n",
    "                non_vocals = batch[\"non_vocals\"].to(device)\n",
    "\n",
    "                v_emb, i_emb = model(vocals, non_vocals)\n",
    "                loss = contrastive_loss(v_emb, i_emb)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # -------------------- LOG + SAVE --------------------\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{epochs} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"âœ… Saved new best model to {save_path}\")\n",
    "\n",
    "    print(\"\\nðŸŽ¯ Training complete!\")\n",
    "    print(f\"Lowest validation loss: {best_val_loss:.4f}\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108967d7",
   "metadata": {},
   "source": [
    "### How to train model (Using GPU)\n",
    "1. If you are running through Google Colab, switch to GPU at the top right hand corner. After that you can start Training.\n",
    "2. If run locally, first install CUDA Toolkit and cuDNN Library.\n",
    "3. Create an environment through anaconda or other means.\n",
    "4. Install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia.\n",
    "5. Create a jupyter kernel and then start the environment through a jupyter notebook.\n",
    "6. Start Training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf3dda",
   "metadata": {},
   "source": [
    "Make sure these prints below returns the correct values to make sure you are using a GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a4dfa",
   "metadata": {},
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2edf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataloaders\n",
    "train_loader, val_loader, _ = create_dataloaders(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70570fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing GTZAN stems from D:\\Users\\ollet\\Downloads\\archive_1\\Data\\genres_stems and originals from D:\\Users\\ollet\\Downloads\\archive_1\\Data\\genres_original\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b2275c2eb84eddb950f86dff98c058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Skipping track reggae.00004 due to loading error: System error.\n",
      "âš ï¸ Skipping track reggae.00005 due to loading error: System error.\n",
      "dict_keys(['vocals', 'non_vocals', 'original', 'chunk_name'])\n",
      "Vocals: torch.Size([4, 1, 64, 751])\n",
      "Non-vocals: torch.Size([4, 1, 64, 751])\n",
      "Original: torch.Size([4, 1, 64, 751])\n",
      "Chunk names: ['metal.00026.chunk1', 'jazz.00067.chunk1', 'hiphop.00074.chunk2', 'metal.00021.chunk1']\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = DualEncoderModel(embedding_dim=128)\n",
    "# Train model\n",
    "history = train_model(model, train_loader, val_loader, epochs=5, lr=1e-4, save_path=\"dual_encoder_best.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepmash)",
   "language": "python",
   "name": "deepmash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
