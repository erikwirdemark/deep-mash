{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62042999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Generator\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "import torchaudio.functional as AF\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from deepmash.data_processing.constants import *\n",
    "from deepmash.data_processing.gtzan_stems import *\n",
    "from deepmash.data_processing.new_dataloader import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6868bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SR = 16000           # think its hard to go lower than 16kHz\n",
    "CHUNK_DURATION_SEC = 15     # all chunks will be exactly this long\n",
    "MIN_CHUNK_DURATION_SEC = 5  # discard chunks shorter than this (otherwise zero-pad to CHUNK_DURATION_SEC)\n",
    "\n",
    "# mel-spectrogram settings (using same as cocola for now)\n",
    "N_MELS = 64\n",
    "F_MIN = 60 \n",
    "F_MAX = 7800\n",
    "WINDOW_SIZE = 1024 # 64ms @ 16kHz (should be power of 2 for efficiency)\n",
    "HOP_SIZE = 320     # 20ms @ 16kHz\n",
    "\n",
    "INPUT_ROOT = \"path/to/gtzan/stems\"\n",
    "ORIGINALS_ROOT = \"path/to/gtzan/originals\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df5fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Utility functions ----------\n",
    "\n",
    "def ensure_same_length(tensors: list[torch.Tensor]) -> list[torch.Tensor]:\n",
    "    min_len = min(len(t) for t in tensors)\n",
    "    return [t[:min_len] for t in tensors]\n",
    "\n",
    "\n",
    "def ensure_2d(x: torch.Tensor) -> torch.Tensor:\n",
    "    if x.ndim == 1:\n",
    "        return x.unsqueeze(0)\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(f\"Input tensor has {x.ndim} dimensions, expected 1 or 2.\")\n",
    "    return x\n",
    "\n",
    "\n",
    "def mix_stems(stems: list[torch.Tensor], peak_val=0.98) -> torch.Tensor:\n",
    "    stems = ensure_same_length(stems)\n",
    "    mixed: torch.Tensor = sum(stems)  # type: ignore\n",
    "    max_val = mixed.abs().max()\n",
    "    if max_val > 0:\n",
    "        mixed = mixed / max_val * peak_val\n",
    "    return mixed\n",
    "\n",
    "\n",
    "def zero_pad_or_clip(x: torch.Tensor, target_len: int) -> torch.Tensor:\n",
    "    if len(x) >= target_len:\n",
    "        return x[:target_len]\n",
    "    pad_len = target_len - len(x)\n",
    "    return torch.cat([x, torch.zeros(pad_len)], dim=0)\n",
    "\n",
    "\n",
    "def load_audio(path: Path | str, sr: int | float, frame_offset=0, num_frames=-1):\n",
    "    y, sr = torchaudio.load(path, frame_offset=frame_offset, num_frames=num_frames)\n",
    "    if y.shape[0] > 1:\n",
    "        y = y.mean(dim=0, keepdim=True)  # convert to mono\n",
    "    y = AF.resample(y, orig_freq=sr, new_freq=TARGET_SR)\n",
    "    return y  # (1, sr*duration)\n",
    "\n",
    "\n",
    "def get_chunks(vocals: torch.Tensor, non_vocals: torch.Tensor) -> Generator[tuple[torch.Tensor, torch.Tensor], None, None]:\n",
    "    chunk_frames = CHUNK_DURATION_SEC * TARGET_SR\n",
    "    min_chunk_frames = MIN_CHUNK_DURATION_SEC * TARGET_SR\n",
    "\n",
    "    vocals, non_vocals = ensure_same_length([vocals, non_vocals])\n",
    "\n",
    "    for i, start in enumerate(range(0, len(vocals), chunk_frames)):\n",
    "        vocals_chunk = vocals[start:start + chunk_frames]\n",
    "        non_vocals_chunk = non_vocals[start:start + chunk_frames]\n",
    "\n",
    "        if len(vocals_chunk) < min_chunk_frames:\n",
    "            continue\n",
    "        vocals_chunk = zero_pad_or_clip(vocals_chunk, chunk_frames)\n",
    "        non_vocals_chunk = zero_pad_or_clip(non_vocals_chunk, chunk_frames)\n",
    "\n",
    "        yield vocals_chunk, non_vocals_chunk\n",
    "\n",
    "\n",
    "def get_gtzan_track_folders(root: Path | str):\n",
    "    return sorted(p for p in Path(root).glob(\"*/*\") if p.is_dir())\n",
    "\n",
    "# ---------- Mel transform ----------\n",
    "\n",
    "class ToLogMel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.to_melspec = AT.MelSpectrogram(\n",
    "            sample_rate=TARGET_SR,\n",
    "            n_mels=N_MELS,\n",
    "            n_fft=WINDOW_SIZE,\n",
    "            hop_length=HOP_SIZE,\n",
    "            f_min=F_MIN,\n",
    "            f_max=F_MAX,\n",
    "        )\n",
    "        self.to_db = AT.AmplitudeToDB()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.to_db(self.to_melspec(x))\n",
    "\n",
    "\n",
    "# ---------- Main dataset with preprocessing ----------\n",
    "\n",
    "class GTZANStemsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir: Path | str = INPUT_ROOT,\n",
    "        originals_root: Path | str = ORIGINALS_ROOT,\n",
    "        preprocess=True,\n",
    "        preprocess_transform: nn.Module | None = None,\n",
    "        runtime_transform: nn.Module | None = None,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self.root = Path(root_dir)\n",
    "        self.originals_root = Path(originals_root)\n",
    "        self.processed_root = self.root.parent / (self.root.name + \"-processed\")\n",
    "\n",
    "        self.preprocess_transform = preprocess_transform\n",
    "        self.runtime_transform = runtime_transform\n",
    "        self.device = device\n",
    "\n",
    "        if preprocess:\n",
    "            print(f\"Preprocessing GTZAN stems from {self.root} and originals from {self.originals_root}\")\n",
    "            self._preprocess()\n",
    "\n",
    "        # After preprocessing, load the chunk list\n",
    "        self.chunk_dirs = sorted([p for p in self.processed_root.glob(\"*\") if p.is_dir()])\n",
    "\n",
    "    def _preprocess(self):\n",
    "        \"\"\"\n",
    "        Assuming input files like \"`self.root`/blues/blues.000001/{drums|bass|other|vocals}.wav\":\n",
    "        1. load as tensors\n",
    "        2. convert to mono if in stereo\n",
    "        3. resample (default 16kHz)\n",
    "        4. mix all non-vocal stems together and discard originals\n",
    "        5. chunk into `CHUNK_DURATION_SEC` (default 10s) segments, zero-pad last chunk if needed\n",
    "        6. apply optional `preprocess_transform` (e.g. mel-spectrogram), make sure shapes are correct\n",
    "        7. save as `self.processed_root`/blues.000001.chunk{1|2|...}/{non-vocals|vocals}.pt\n",
    "        \"\"\"\n",
    "        os.makedirs(self.processed_root, exist_ok=True)\n",
    "        track_folders = get_gtzan_track_folders(self.root)\n",
    "\n",
    "        for track_folder in tqdm(track_folders):\n",
    "            all_stem_paths = list(track_folder.glob(\"*.wav\"))\n",
    "            assert {p.stem for p in all_stem_paths} == {\"drums\", \"bass\", \"other\", \"vocals\"}, \\\n",
    "                f\"Missing stems for {track_folder}\"\n",
    "\n",
    "            vocals_path = [p for p in all_stem_paths if p.stem == \"vocals\"][0]\n",
    "            non_vocals_paths = [p for p in all_stem_paths if p.stem != \"vocals\"]\n",
    "\n",
    "            track_name = track_folder.name\n",
    "            genre = track_folder.parent.name  # e. g. \"blues\"\n",
    "            orig_path = self.originals_root / f\"{genre}\" / f\"{track_name}.wav\"\n",
    "            \n",
    "            # Load and mix stems\n",
    "            try:\n",
    "                vocals = load_audio(vocals_path, sr=TARGET_SR)\n",
    "                non_vocals = mix_stems([load_audio(p, sr=TARGET_SR) for p in non_vocals_paths])\n",
    "                original = load_audio(orig_path, sr=TARGET_SR)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipping track {track_name} due to loading error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            vocals = vocals.squeeze(0)\n",
    "            non_vocals = non_vocals.squeeze(0)\n",
    "            original = original.squeeze(0)\n",
    "\n",
    "            # Generate aligned chunks\n",
    "            for i, ((vocals_chunk, non_vocals_chunk), (orig_chunk, _)) in enumerate(\n",
    "                zip(get_chunks(vocals, non_vocals), get_chunks(original, original))\n",
    "            ):\n",
    "                if self.preprocess_transform is not None:\n",
    "                    with torch.no_grad():\n",
    "                        vocals_chunk = self.preprocess_transform(vocals_chunk.unsqueeze(0))  # (1, T)\n",
    "                        non_vocals_chunk = self.preprocess_transform(non_vocals_chunk.unsqueeze(0))\n",
    "                        orig_chunk = self.preprocess_transform(orig_chunk.unsqueeze(0))\n",
    "                        \n",
    "                chunk_folder = self.processed_root / f\"{track_name}.chunk{i+1}\"\n",
    "                os.makedirs(chunk_folder, exist_ok=True)\n",
    "                torch.save(vocals_chunk, chunk_folder / \"vocals.pt\")\n",
    "                torch.save(non_vocals_chunk, chunk_folder / \"non-vocals.pt\")\n",
    "                torch.save(orig_chunk, chunk_folder / \"original.pt\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.chunk_dirs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_dir = self.chunk_dirs[idx]\n",
    "        vocals = torch.load(chunk_dir / \"vocals.pt\")\n",
    "        non_vocals = torch.load(chunk_dir / \"non-vocals.pt\") \n",
    "        original = torch.load(chunk_dir / \"original.pt\")\n",
    "\n",
    "        if self.runtime_transform:\n",
    "            vocals = self.runtime_transform(vocals)\n",
    "            non_vocals = self.runtime_transform(non_vocals)\n",
    "            original = self.runtime_transform(original)\n",
    "\n",
    "        return {\n",
    "            \"vocals\": vocals.to(self.device),\n",
    "            \"non_vocals\": non_vocals.to(self.device),\n",
    "            \"original\": original.to(self.device),\n",
    "            \"chunk_name\": chunk_dir.name,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd7b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Split dataset by tracks ----------\n",
    "\n",
    "def split_dataset_by_tracks(dataset: GTZANStemsDataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, random_state=42):\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6\n",
    "\n",
    "    # Extract unique track names\n",
    "    all_tracks = sorted({p.name.split(\".chunk\")[0] for p in dataset.chunk_dirs})\n",
    "    \n",
    "    train_tracks, temp_tracks = train_test_split(all_tracks, test_size=(1 - train_ratio), random_state=random_state)\n",
    "    val_tracks, test_tracks = train_test_split(temp_tracks, test_size=test_ratio / (test_ratio + val_ratio), random_state=random_state)\n",
    "\n",
    "    def get_chunk_indices(track_list):\n",
    "        return [i for i, chunk_dir in enumerate(dataset.chunk_dirs) if chunk_dir.name.split(\".chunk\")[0] in track_list]\n",
    "\n",
    "    train_dataset = Subset(dataset, get_chunk_indices(train_tracks))\n",
    "    val_dataset = Subset(dataset, get_chunk_indices(val_tracks))\n",
    "    test_dataset = Subset(dataset, get_chunk_indices(test_tracks))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# ---------- DataLoaders ----------\n",
    "\n",
    "def create_dataloaders(preprocess=False, batch_size=16, num_workers=2):\n",
    "    dataset = GTZANStemsDataset(preprocess=preprocess, preprocess_transform=ToLogMel())\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset = split_dataset_by_tracks(dataset)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, _ = create_dataloaders(batch_size=4)\n",
    "# Get one batch from the training dataloader\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Print keys\n",
    "print(batch.keys())\n",
    "print(\"Vocals:\", batch['vocals'].shape) # [batch_size, channels, n_mels, time] if using Mel\n",
    "print(\"Non-vocals:\", batch['non_vocals'].shape)\n",
    "print(\"Original:\", batch['original'].shape)\n",
    "print(\"Chunk names:\", batch['chunk_name'])   # List of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d952557b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNTransformer(nn.Module):\n",
    "    def __init__(self, cnn_name=\"efficientnet_b0\", transformer_dim=512, nhead=8, num_layers=4, device=\"gpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        \n",
    "        # CNN backbone\n",
    "        self.cnn = timm.create_model(cnn_name, pretrained=True, in_chans=1, num_classes=0, global_pool=\"avg\")\n",
    "        cnn_out_dim = self.cnn.num_features\n",
    "        \n",
    "        # Project CNN features to transformer dimension\n",
    "        self.fc_proj = nn.Linear(cnn_out_dim, transformer_dim)\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=transformer_dim, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # Final embedding projection\n",
    "        self.embedding = nn.Linear(transformer_dim, transformer_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B, 1, T, F) -> Mel spectrogram\n",
    "        \"\"\"\n",
    "        B = x.size(0)\n",
    "        # Flatten spectrogram to 2D image for CNN: (B, 1, T, F)\n",
    "        cnn_feat = self.cnn(x)  # (B, cnn_out_dim)\n",
    "        proj_feat = self.fc_proj(cnn_feat).unsqueeze(1)  # (B, 1, transformer_dim)\n",
    "        trans_feat = self.transformer(proj_feat)  # (B, 1, transformer_dim)\n",
    "        emb = self.embedding(trans_feat[:, 0, :])  # (B, transformer_dim)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        vocals = batch[\"vocals\"].to(device)\n",
    "        non_vocals = batch[\"non_vocals\"].to(device)\n",
    "        original = batch[\"original\"].to(device)\n",
    "        \n",
    "        # Forward\n",
    "        emb_vocals = model(vocals)\n",
    "        emb_non_vocals = model(non_vocals)\n",
    "        emb_mix = model(original)\n",
    "        \n",
    "        # Combine vocals + non-vocals for predicted mix embedding\n",
    "        pred_emb = (emb_vocals + emb_non_vocals) / 2.0\n",
    "        \n",
    "        target = torch.ones(pred_emb.size(0), device=device)  # batch size\n",
    "        loss = criterion(pred_emb, emb_mix, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            vocals = batch[\"vocals\"].to(device)\n",
    "            non_vocals = batch[\"non_vocals\"].to(device)\n",
    "            original = batch[\"original\"].to(device)\n",
    "            \n",
    "            emb_vocals = model(vocals)\n",
    "            emb_non_vocals = model(non_vocals)\n",
    "            emb_mix = model(original)\n",
    "            pred_emb = (emb_vocals + emb_non_vocals) / 2.0\n",
    "            \n",
    "            loss = criterion(pred_emb, emb_mix)\n",
    "            running_loss += loss.item()\n",
    "    return running_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108967d7",
   "metadata": {},
   "source": [
    "## To run this - use GPU\n",
    "1. If run locally, first install CUDA Toolkit and cuDNN Library.\n",
    "2. Create an environment through anaconda or other means.\n",
    "3. Install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia.\n",
    "4. Create a jupyter kernel and then start the environment through a jupyter notebook.\n",
    "5. Start Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2edf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataloaders\n",
    "train_loader, val_loader, _ = create_dataloaders(batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70570fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing GTZAN stems from D:\\Users\\ollet\\Downloads\\archive_1\\Data\\genres_stems and originals from D:\\Users\\ollet\\Downloads\\archive_1\\Data\\genres_original\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b2275c2eb84eddb950f86dff98c058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping track reggae.00004 due to loading error: System error.\n",
      "⚠️ Skipping track reggae.00005 due to loading error: System error.\n",
      "dict_keys(['vocals', 'non_vocals', 'original', 'chunk_name'])\n",
      "Vocals: torch.Size([4, 1, 64, 751])\n",
      "Non-vocals: torch.Size([4, 1, 64, 751])\n",
      "Original: torch.Size([4, 1, 64, 751])\n",
      "Chunk names: ['metal.00026.chunk1', 'jazz.00067.chunk1', 'hiphop.00074.chunk2', 'metal.00021.chunk1']\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Number of GPUs\n",
    "print(torch.cuda.get_device_name(0))  # Name of the first GPU\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "# Initialize model\n",
    "print(f\"Using device: {device}\")\n",
    "model = CNNTransformer(device=device).to(device)\n",
    "    \n",
    "# Cosine embedding loss\n",
    "criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "print(\"Starting training...\")\n",
    "num_epochs = 10\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "# Save model\n",
    "torch.save(model.state_dict(), \"cnn_transformer_model.pt\")\n",
    "print(\"Model saved to cnn_transformer_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepmash)",
   "language": "python",
   "name": "deepmash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
