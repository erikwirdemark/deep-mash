{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBElEx3/Wz8MQ633llBJzj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erikwirdemark/deep-mash/blob/main/deep_mash.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GTZAN Stems DataLoader\n",
        "==========================================================\n",
        "Organized into separate sections for easy navigation:\n",
        "1. Audio Processing Utilities\n",
        "2. Dataset Discovery\n",
        "3. Dataset Class\n",
        "4. DataLoader Creation\n"
      ],
      "metadata": {
        "id": "plc_O3ewlKlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import soundfile as sf\n",
        "from typing import Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "U9Qu1uL4kcKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Audio Processing Utilities"
      ],
      "metadata": {
        "id": "mNBLiYnIlQL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioProcessor:\n",
        "    \"\"\"Handles all audio loading, mixing, and spectrogram computation.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        sample_rate: int = 22050,\n",
        "        n_fft: int = 2048,\n",
        "        hop_length: int = 512,\n",
        "        n_mels: int = 128\n",
        "    ):\n",
        "        self.sample_rate = sample_rate\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "\n",
        "    def load_audio(self, file_path: Path) -> np.ndarray:\n",
        "        \"\"\"Load audio file at target sample rate.\"\"\"\n",
        "        audio, sr = librosa.load(file_path, sr=self.sample_rate, mono=True)\n",
        "        return audio\n",
        "\n",
        "    def mix_stems(self, stem_paths: List[Path]) -> np.ndarray:\n",
        "        \"\"\"Mix multiple stems into single audio.\"\"\"\n",
        "        stems = [self.load_audio(path) for path in stem_paths]\n",
        "\n",
        "        # Ensure same length\n",
        "        min_len = min(len(stem) for stem in stems)\n",
        "        stems = [stem[:min_len] for stem in stems]\n",
        "\n",
        "        # Mix with equal weights\n",
        "        mixed = sum(stems)\n",
        "\n",
        "        # Normalize to prevent clipping\n",
        "        max_val = np.abs(mixed).max()\n",
        "        if max_val > 0:\n",
        "            mixed = mixed / max_val * 0.9\n",
        "\n",
        "        return mixed\n",
        "\n",
        "    def extract_segment(\n",
        "        self,\n",
        "        audio: np.ndarray,\n",
        "        duration: float,\n",
        "        offset: Optional[float] = None\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"Extract fixed-duration segment from audio.\"\"\"\n",
        "        target_length = int(duration * self.sample_rate)\n",
        "\n",
        "        if len(audio) < target_length:\n",
        "            # Pad if too short\n",
        "            audio = np.pad(audio, (0, target_length - len(audio)), mode='constant')\n",
        "        elif len(audio) > target_length:\n",
        "            # Extract segment\n",
        "            if offset is not None:\n",
        "                start = int(offset * self.sample_rate)\n",
        "            else:\n",
        "                max_start = len(audio) - target_length\n",
        "                start = np.random.randint(0, max(1, max_start))\n",
        "            audio = audio[start:start + target_length]\n",
        "\n",
        "        return audio\n",
        "\n",
        "    def compute_melspectrogram(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute normalized log mel spectrogram.\"\"\"\n",
        "        # Compute mel spectrogram\n",
        "        mel_spec = librosa.feature.melspectrogram(\n",
        "            y=audio,\n",
        "            sr=self.sample_rate,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop_length,\n",
        "            n_mels=self.n_mels,\n",
        "            fmax=8000\n",
        "        )\n",
        "\n",
        "        # Convert to log scale (dB)\n",
        "        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
        "\n",
        "        # Normalize to [0, 1]\n",
        "        log_mel_spec = (log_mel_spec + 80) / 80\n",
        "        log_mel_spec = np.clip(log_mel_spec, 0, 1)\n",
        "\n",
        "        return log_mel_spec\n",
        "\n",
        "\n",
        "class AudioAugmentor:\n",
        "    \"\"\"Handles audio augmentation for training.\"\"\"\n",
        "\n",
        "    def __init__(self, sample_rate: int = 22050):\n",
        "        self.sample_rate = sample_rate\n",
        "\n",
        "    def augment(self, audio: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Apply random augmentations.\"\"\"\n",
        "        # Random time stretch (±10%)\n",
        "        if np.random.rand() > 0.5:\n",
        "            rate = np.random.uniform(0.9, 1.1)\n",
        "            audio = librosa.effects.time_stretch(audio, rate=rate)\n",
        "\n",
        "        # Random pitch shift (±2 semitones)\n",
        "        if np.random.rand() > 0.5:\n",
        "            n_steps = np.random.uniform(-2, 2)\n",
        "            audio = librosa.effects.pitch_shift(\n",
        "                audio, sr=self.sample_rate, n_steps=n_steps\n",
        "            )\n",
        "\n",
        "        # Random gain (±3 dB)\n",
        "        if np.random.rand() > 0.5:\n",
        "            gain_db = np.random.uniform(-3, 3)\n",
        "            audio = audio * (10 ** (gain_db / 20))\n",
        "\n",
        "        return audio"
      ],
      "metadata": {
        "id": "yvOomGmDlAGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Dataset Discovery"
      ],
      "metadata": {
        "id": "E3uDsGkDlXoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StemDiscovery:\n",
        "    \"\"\"Discovers and validates GTZAN stem files.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def find_tracks(data_dir: Path, original_tracks_dir: Optional[Path] = None) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Find all valid tracks with required stems and original mixed audio.\n",
        "        Expected structure:\n",
        "          - data_dir/genre/track_name/*.wav (stems)\n",
        "          - original_tracks_dir/genre/track_name.wav (original mix)\n",
        "\n",
        "        Args:\n",
        "            data_dir: Path to GTZAN stems directory\n",
        "            original_tracks_dir: Path to original (unseparated) tracks\n",
        "\n",
        "        Returns:\n",
        "            List of track dictionaries with stem paths and original track path\n",
        "        \"\"\"\n",
        "        tracks = []\n",
        "        genre_dirs = [d for d in data_dir.iterdir() if d.is_dir()]\n",
        "\n",
        "        for genre_dir in genre_dirs:\n",
        "            track_dirs = [d for d in genre_dir.iterdir() if d.is_dir()]\n",
        "\n",
        "            for track_dir in track_dirs:\n",
        "                stems = StemDiscovery._find_stems_in_directory(track_dir)\n",
        "\n",
        "                # Find original mixed track\n",
        "                original_path = None\n",
        "                if original_tracks_dir is not None:\n",
        "                    original_path = StemDiscovery._find_original_track(\n",
        "                        original_tracks_dir, genre_dir.name, track_dir.name\n",
        "                    )\n",
        "\n",
        "                # Only add track if vocal stem is present and original exists\n",
        "                if stems['vocals'] is not None and original_path is not None:\n",
        "                    tracks.append({\n",
        "                        'track_name': track_dir.name,\n",
        "                        'genre': genre_dir.name,\n",
        "                        'stems': stems,\n",
        "                        'original': original_path\n",
        "                    })\n",
        "\n",
        "        return tracks\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_stems_in_directory(track_dir: Path) -> Dict[str, Optional[Path]]:\n",
        "        \"\"\"Find stem files in a track directory.\"\"\"\n",
        "        stems = {\n",
        "            'vocals': None,\n",
        "            'drums': None,\n",
        "            'bass': None,\n",
        "            'other': None\n",
        "        }\n",
        "\n",
        "        for stem_file in track_dir.glob('*.wav'):\n",
        "            stem_name = stem_file.stem.lower()\n",
        "            if 'vocal' in stem_name:\n",
        "                stems['vocals'] = stem_file\n",
        "            elif 'drum' in stem_name:\n",
        "                stems['drums'] = stem_file\n",
        "            elif 'bass' in stem_name:\n",
        "                stems['bass'] = stem_file\n",
        "            elif 'other' in stem_name or 'accomp' in stem_name:\n",
        "                stems['other'] = stem_file\n",
        "\n",
        "        return stems\n",
        "\n",
        "    @staticmethod\n",
        "    def _find_original_track(\n",
        "        original_dir: Path,\n",
        "        genre: str,\n",
        "        track_name: str\n",
        "    ) -> Optional[Path]:\n",
        "        \"\"\"Find the original (unseparated) track.\"\"\"\n",
        "        # Try different possible locations\n",
        "        possible_paths = [\n",
        "            original_dir / genre / f\"{track_name}.wav\",\n",
        "            original_dir / genre / f\"{track_name}.mp3\",\n",
        "            original_dir / f\"{track_name}.wav\",\n",
        "            original_dir / f\"{track_name}.mp3\",\n",
        "        ]\n",
        "\n",
        "        for path in possible_paths:\n",
        "            if path.exists():\n",
        "                return path\n",
        "\n",
        "        return None"
      ],
      "metadata": {
        "id": "Z85bhidZlEi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. DATASET CLASS"
      ],
      "metadata": {
        "id": "6aMd7X1bla57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GTZANStemsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for vocal-instrumental matching from GTZAN Stems.\n",
        "\n",
        "    Returns:\n",
        "        - Vocal spectrogram (isolated vocal stem)\n",
        "        - Full track spectrogram (original mixed track - ground truth)\n",
        "        - Track index (for positive pair identification)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        stems_dir: str,\n",
        "        original_tracks_dir: str,\n",
        "        sample_rate: int = 22050,\n",
        "        n_fft: int = 2048,\n",
        "        hop_length: int = 512,\n",
        "        n_mels: int = 128,\n",
        "        duration: float = 10.0,\n",
        "        segment_offset: Optional[float] = None,\n",
        "        augment: bool = True,\n",
        "        cache_spectrograms: bool = False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            stems_dir: Path to GTZAN stems directory\n",
        "            original_tracks_dir: Path to original (unseparated) tracks directory\n",
        "            sample_rate: Target sample rate\n",
        "            n_fft: FFT window size\n",
        "            hop_length: Hop length for STFT\n",
        "            n_mels: Number of mel bands\n",
        "            duration: Duration of audio segments in seconds\n",
        "            segment_offset: Fixed offset for reproducibility (None = random)\n",
        "            augment: Whether to apply data augmentation\n",
        "            cache_spectrograms: Whether to cache computed spectrograms\n",
        "        \"\"\"\n",
        "        self.stems_dir = Path(stems_dir)\n",
        "        self.original_tracks_dir = Path(original_tracks_dir)\n",
        "        self.duration = duration\n",
        "        self.segment_offset = segment_offset\n",
        "        self.augment = augment\n",
        "        self.cache_spectrograms = cache_spectrograms\n",
        "\n",
        "        # Initialize processors\n",
        "        self.audio_processor = AudioProcessor(\n",
        "            sample_rate=sample_rate,\n",
        "            n_fft=n_fft,\n",
        "            hop_length=hop_length,\n",
        "            n_mels=n_mels\n",
        "        )\n",
        "        self.augmentor = AudioAugmentor(sample_rate=sample_rate) if augment else None\n",
        "\n",
        "        # Discover tracks\n",
        "        self.tracks = StemDiscovery.find_tracks(\n",
        "            self.stems_dir,\n",
        "            self.original_tracks_dir\n",
        "        )\n",
        "        print(f\"Found {len(self.tracks)} tracks with vocal stems and original audio\")\n",
        "\n",
        "        # Cache for spectrograms\n",
        "        self.cache = {} if cache_spectrograms else None\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.tracks)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int]:\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "            vocal_spec: (1, n_mels, time_steps) - isolated vocal stem\n",
        "            full_track_spec: (1, n_mels, time_steps) - original mixed track (ground truth)\n",
        "            track_idx: int for identifying positive pairs\n",
        "        \"\"\"\n",
        "        # Check cache\n",
        "        cache_key = f\"{idx}_{self.segment_offset}\"\n",
        "        if self.cache_spectrograms and cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        track = self.tracks[idx]\n",
        "\n",
        "        # Process vocal (isolated stem)\n",
        "        vocal = self._process_vocal(track)\n",
        "\n",
        "        # Process original full track (ground truth)\n",
        "        full_track = self._process_full_track(track)\n",
        "\n",
        "        # Compute spectrograms\n",
        "        vocal_spec = self.audio_processor.compute_melspectrogram(vocal)\n",
        "        full_track_spec = self.audio_processor.compute_melspectrogram(full_track)\n",
        "\n",
        "        # Convert to tensors\n",
        "        vocal_spec = torch.FloatTensor(vocal_spec).unsqueeze(0)\n",
        "        full_track_spec = torch.FloatTensor(full_track_spec).unsqueeze(0)\n",
        "\n",
        "        result = (vocal_spec, full_track_spec, idx)\n",
        "\n",
        "        # Cache if enabled\n",
        "        if self.cache_spectrograms:\n",
        "            self.cache[cache_key] = result\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _process_vocal(self, track: Dict) -> np.ndarray:\n",
        "        \"\"\"Load and process vocal stem.\"\"\"\n",
        "        vocal = self.audio_processor.load_audio(track['stems']['vocals'])\n",
        "        vocal = self.audio_processor.extract_segment(\n",
        "            vocal, self.duration, self.segment_offset\n",
        "        )\n",
        "        if self.augment:\n",
        "            vocal = self.augmentor.augment(vocal)\n",
        "        return vocal\n",
        "\n",
        "    def _process_full_track(self, track: Dict) -> np.ndarray:\n",
        "        \"\"\"Load and process original full mixed track (ground truth).\"\"\"\n",
        "        full_track = self.audio_processor.load_audio(track['original'])\n",
        "        full_track = self.audio_processor.extract_segment(\n",
        "            full_track, self.duration, self.segment_offset\n",
        "        )\n",
        "        if self.augment:\n",
        "            full_track = self.augmentor.augment(full_track)\n",
        "        return full_track"
      ],
      "metadata": {
        "id": "qkwBnyN9lG7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. DATALOADER CREATION"
      ],
      "metadata": {
        "id": "FxV_xc4cleTy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBcQSSVJiVfh"
      },
      "outputs": [],
      "source": [
        "def collate_contrastive_batch(batch):\n",
        "    \"\"\"\n",
        "    Collate function for contrastive learning.\n",
        "\n",
        "    Returns:\n",
        "        vocal_specs: (batch_size, 1, n_mels, time_steps) - vocal stems\n",
        "        full_track_specs: (batch_size, 1, n_mels, time_steps) - original tracks (ground truth)\n",
        "        labels: (batch_size,) track indices for positive pairs\n",
        "    \"\"\"\n",
        "    vocal_specs = torch.stack([item[0] for item in batch])\n",
        "    full_track_specs = torch.stack([item[1] for item in batch])\n",
        "    labels = torch.LongTensor([item[2] for item in batch])\n",
        "\n",
        "    return vocal_specs, full_track_specs, labels\n",
        "\n",
        "\n",
        "def create_dataloaders(\n",
        "    stems_dir: str,\n",
        "    original_tracks_dir: str,\n",
        "    batch_size: int = 32,\n",
        "    train_split: float = 0.8,\n",
        "    val_split: float = 0.1,\n",
        "    num_workers: int = 4,\n",
        "    seed: int = 42,\n",
        "    **dataset_kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Create train, validation, and test dataloaders.\n",
        "\n",
        "    Args:\n",
        "        stems_dir: Path to GTZAN stems directory\n",
        "        original_tracks_dir: Path to original (unseparated) tracks directory\n",
        "        batch_size: Batch size for dataloaders\n",
        "        train_split: Proportion of data for training\n",
        "        val_split: Proportion of data for validation\n",
        "        num_workers: Number of worker processes\n",
        "        seed: Random seed for reproducibility\n",
        "        **dataset_kwargs: Additional arguments for GTZANStemsDataset\n",
        "\n",
        "    Returns:\n",
        "        train_loader, val_loader, test_loader\n",
        "    \"\"\"\n",
        "    # Create full dataset\n",
        "    dataset = GTZANStemsDataset(stems_dir, original_tracks_dir, **dataset_kwargs)\n",
        "\n",
        "    # Split dataset\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(train_split * total_size)\n",
        "    val_size = int(val_split * total_size)\n",
        "    test_size = total_size - train_size - val_size\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        dataset, [train_size, val_size, test_size]\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_contrastive_batch,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_contrastive_batch,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        collate_fn=collate_contrastive_batch,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset splits:\")\n",
        "    print(f\"  Train: {len(train_dataset)} samples\")\n",
        "    print(f\"  Val: {len(val_dataset)} samples\")\n",
        "    print(f\"  Test: {len(test_dataset)} samples\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Create dataloaders\n",
        "    stems_dir = \"/path/to/gtzan-stems\"  # Directory with separated stems\n",
        "    original_tracks_dir = \"/path/to/gtzan-original\"  # Directory with original mixed tracks\n",
        "\n",
        "    train_loader, val_loader, test_loader = create_dataloaders(\n",
        "        stems_dir=stems_dir,\n",
        "        original_tracks_dir=original_tracks_dir,\n",
        "        batch_size=16,\n",
        "        sample_rate=22050,\n",
        "        duration=10.0,\n",
        "        augment=True,\n",
        "        cache_spectrograms=False\n",
        "    )\n",
        "\n",
        "    # Test loading a batch\n",
        "    print(\"\\nTesting batch loading...\")\n",
        "    vocal_batch, full_track_batch, labels = next(iter(train_loader))\n",
        "    print(f\"Vocal batch shape: {vocal_batch.shape}\")\n",
        "    print(f\"Full track batch shape: {full_track_batch.shape}\")\n",
        "    print(f\"Labels shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "D-U7tyiwnTTS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}